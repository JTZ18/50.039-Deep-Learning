{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SegFormer Inferencce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from src.utils.utils import *\n",
    "\n",
    "def set_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "  torch.backends.cudnn.deterministic = True\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.segFormer import SegFormer\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import *\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch import nn\n",
    "\n",
    "val_transforms = A.Compose(\n",
    "    [\n",
    "        A.Resize(height=256, width=256),\n",
    "        A.Normalize(\n",
    "            mean=[0.0, 0.0, 0.0],\n",
    "            std=[1.0, 1.0, 1.0],\n",
    "            max_pixel_value=255.0,\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Loading checkpoint\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_FILENAME = \"./checkpoints/segformer-checkpoints/checkpoint_7.pth.tar\"\n",
    "\n",
    "model = SegFormer(\n",
    "  in_channels=3,\n",
    "  widths=[64, 128, 256, 512],\n",
    "  depths=[3, 4, 6, 3],\n",
    "  all_num_heads=[1, 2, 4, 8],\n",
    "  patch_sizes=[7, 3, 3, 3],\n",
    "  overlap_sizes=[4, 2, 2, 2],\n",
    "  reduction_ratios=[8, 4, 2, 1],\n",
    "  mlp_expansions=[4, 4, 4, 4],\n",
    "  decoder_channels=256,\n",
    "  scale_factors=[8, 4, 2, 1],\n",
    "  num_classes=1,\n",
    "  drop_prob=0.3,\n",
    ").to(device)\n",
    "\n",
    "checkpoint = torch.load(CHECKPOINT_FILENAME)\n",
    "model, _, _ = load_checkpoint(checkpoint, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_inference(model, image_path, device=\"cuda\"):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    orig_size = image.size\n",
    "\n",
    "    # Apply the transformations\n",
    "    image = val_transforms(image=np.array(image))['image']\n",
    "\n",
    "    # Add an extra dimension for the batch size\n",
    "    image = image.unsqueeze(0)\n",
    "\n",
    "    # Move the image tensor to the device\n",
    "    image = image.to(device)\n",
    "\n",
    "    # Run the image tensor through the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "\n",
    "    # Apply the sigmoid function and threshold at 0.5\n",
    "    preds = torch.sigmoid(preds)\n",
    "    preds = (preds > 0.5).float()\n",
    "\n",
    "    # Remove the batch dimension and channel dimension\n",
    "    preds = preds.squeeze().cpu()\n",
    "\n",
    "    # Add back the batch dimension and channel dimension\n",
    "    preds = preds.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Resize the prediction back to the original size\n",
    "    preds = F.interpolate(preds, size=(orig_size[1], orig_size[0]), mode=\"nearest\")\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    preds_np = preds.squeeze().numpy()\n",
    "\n",
    "    # Convert the numpy array to a PIL Image and display it\n",
    "    preds_img = Image.fromarray((preds_np * 255).astype(np.uint8))\n",
    "    # preds_img.show()\n",
    "\n",
    "    # Save the image to the specified filepath\n",
    "    filename = os.path.basename(image_path)\n",
    "    output_path = os.path.join(\"./streamlit-app/data/prediction/\", filename)\n",
    "    preds_img.save(output_path)\n",
    "\n",
    "    return preds\n",
    "\n",
    "run_inference(model, \"./streamlit-app/data/sample/ISIC_0036333.jpg\", device=device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
